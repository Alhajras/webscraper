\chapter{Conclusions and Future Work}\label{chap:conclusion}

Within this chapter, we address the questions initially introduced at the opening of the thesis (Section \ref{sec:contribution}), conclude our experiments, and open the door for additional research to enhance the solution and explore areas that have not been previously examined.
\section{Conclusions}

It is crucial to remember the primary questions and contributions intended to be accomplished by this thesis and whether we have successfully addressed them or if they remain abstract. In the following list, we iterate through the contributions mentioned in Section \ref{sec:contribution}  and discuss our final findings.

\begin{itemize}
  \item \textit{What are the challenges and bottlenecks to creating a scalable, configurable search engine?}
Two primary bottlenecks restrict the crawler's performance. The first is the loading time, or the time it takes to render a page, which varies for each page and ultimately affects the rate at which the crawler can make requests. The second challenge is that websites can block the crawler if the request rate increases, which is also contingent on the configurations of the websites' firewalls.

\item \textit{Can we outperform a similar tool like ParseHub?}
ParseHub demonstrated an advantage with its smoother workflow and improved automated field selection. Nonetheless, Scriburg surpassed ParseHub in terms of performance and overall robustness.

\item \textit{How does changing the configurations provided by the user interface affect the results in the crawling and indexing accuracy?} The user interface offers forms, including optional and advanced crawling and indexing options. The used options deliver a helpful and adaptable user experience. Even users with limited programming experience can easily configure and execute straightforward crawling and indexing tasks by relying on the default values provided.

    \item \textit{Can we create a decent User Interface UI that intuitively allows users to crawl and index targeted websites from the internet?} Yes, we did. However, manually adding and removing inspectors was time-consuming and should be refined.
    
\item \textit{How well do crawlers react to different websites with different DOM structures?} While the internet hosts millions of websites, making it impossible to ensure coverage for all possible use cases, we have evaluated many websites and effectively managed their diverse implementations.

    \item \textit{Can we integrate the indexing and crawling processes in the same tool?} It was possible to provide a user-friendly interface that combines both functionalities.
    
    \item \textit{Can we find meaningful evaluation metrics for the implemented search engine?} We have discovered that evaluating crawling can pose challenges, but certain aspects can be addressed to provide insights into the crawler's performance.
    
\end{itemize}

\section{Future Work}
While evaluating \textbf{Scriburg}, the most wanted feature was to automate the inspector's selections. The solution should be similar to what \textbf{ParseHub} is implementing. The user should be faced with a live session where they can click on any title or price they want to collect simply by clicking. However, This feature is not easy to implement and can take up to \textbf{three months} to perfect. 

Adding \textbf{IP Rotation} is highly recommended because although the crawling rate was not high in the evaluation, the crawler got banned twice from different websites. Implementing IP Rotation is relatively easy. Some services provide free proxy list API\footnote{SSL Proxies: \url{https://www.sslproxies.org/}} to be used to make proxied requests. One can add each proxy bypassing the flag \texttt{proxy-server} in Selenium. This feature can take up to \textbf{three days} to implement and test.

Adding a \textbf{Steps}\footnote{PrimeNG Steps: \url{https://primeng.org/steps/personal}} component can allow the users to crawl and index step by step. For example, the first step is to create a project name like Stack Overflow. This name will be used for all templates, runners, crawlers, and indexers instead of entering the name in each form. The steps will make running a crawler intuitive and reduce user confusion. Using the \textbf{Steps} component from \textbf{PrimeNg} will take \textbf{two weeks} to implement and add this functionality. This feature can solve the issue by making the user interface more intuitive and easily used.

Early stopping the crawlers once they are inefficient helps save resources. This can be due to an internet connection; the website is down, the crawler needs to be better configured, and more issues can make crawling inefficient. Although we are serving valuable information to help monitor the crawling process, for non-technical users, it can be hard to understand what the HTTP status codes stand for. To fix this, we can set up some rules to convert errors into valuable messages that users can understand. This feature can take between \textbf{two weeks} to \textbf{one month} to implement.

Leveraging the \texttt{robots.txt} file to direct web crawlers can be effective, but there is a need for further research into establishing a robust protocol for crawler behavior. For instance, it proves beneficial for crawlers to gain insights into website characteristics, such as estimating the number of links, products, or items present. This information helps users in determining the appropriate scaling of their crawlers. Additionally, the file can specify the maximum allowable requests per second to prevent potential denial-of-service (DoS) issues. It could also contain preferences regarding crawl timings, such as restricting activity to nighttime.
