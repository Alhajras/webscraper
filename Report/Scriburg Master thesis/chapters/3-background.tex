\chapter{Background}\
\label{chap:background}
This chapter tackles the fundamental principles and groundwork of the theory encompassing concepts, terminology, and methodologies related to search engines as applied within this thesis. Section \ref{sec:web-search-engine} dives into the essential components and characteristics required to implement the search engine discussed in this thesis. Section \ref{sec:crawler} provides a comprehensive examination of the crawler's specifications and architecture. Section \ref{sec:indexing} offers an in-depth explanation of the fundamental indexing terms and concepts essential to this thesis, while Section \ref{sec:ranking} explores the ranking score used in this research.


\section{Web Search Engine}
\label{sec:web-search-engine}
Web search engine is software that collects information from the web and indexes it efficiently to optimize the searching process by the end user. When users enter their queries to ask for information, the engine performs queries, looks up the pre-built organized index, and returns relevant results. \textbf{Search Engine Results Pages (SERPs)}, present the returned results from a search. The result is then ranked based on predefined criteria. 

Web search engines use \textit{crawlers} or \textit{spiders} to collect and harvest the internet, jumping from one page to another. Each page can contain several links. The crawler's task is to find the links, visit them, and harvest them. Followed by crawlers, indexing is the next process where information is organized and optimized for search.

\begin{figure}[h]	
     \centering
         \includegraphics[width=9cm]{figures/engine_components.png}
              \caption{An overview of a generic search engine system \cite{lmu2014}}
     \label{fig:search-engine-overvoew}
\end{figure}

\subsection{Requirements and Features}
Regardless of all the search engines' implementation and design, they share certain features and prerequisites for their effectiveness. Below is a compilation of the most essential features: 

\begin{itemize}
	\item[] \textbf{Web Crawling and Indexing:} As shown in Figure \ref{fig:search-engine-overvoew}, the initial step in the search engine's operation is web crawling. Crawlers initiate the process by connecting to the web and downloading the required pages. Subsequently, indexing comes into play, where the downloaded files are organized and indexed to enhance querying and search efficiency. Parsing the downloaded pages can be carried out in either the crawling phase or during indexing. In Scriburg, this parsing occurs during the crawling process. It is worth noting that, in Scriburg, pages are not downloaded; the targeted documents are parsed and stored in the database, and the pages are discarded immediately. We only store the required information from the page, not the entire page. More explanation on crawling and indexing will be discussed in sections \ref{sec:crawler} and \ref{sec:indexing}.
  
  \item[] \textbf{Ranking and Relevancy:} As indicated in Figure \ref{fig:search-engine-overvoew}, when users input a query to search for relevant documents, they face the \textbf{Search Engine Results Pages (SERP)}. Users typically focus on the top results while overlooking the lower results. Hence, we must maintain relevancy. Ensuring relevance is a challenging task. Ranking the discovered documents and prioritizing the most relevant documents at the top and the less relevant ones further down is crucial. More explanation on ranking will be discussed in section \ref{sec:ranking}.
  
  \item[] \textbf{Scalability and Performance:} A distributed system is essential for managing the extensive data and traffic demands. A \textbf{load balancer} is critical in distributing the crawling tasks efficiently among nodes and threads. In this, we will discuss the implemented loading balance mechanism to distribute the crawling tasks among the crawlers. More information on distributing the workload can be found in section \ref{sec:threads-pool}.

\end{itemize}

\section{Crawler}
\label{sec:crawler}
The essential role of crawlers is to effectively and reliably collect as much information from the web as possible. There are different types and categories of crawlers. The first category is  \textbf{Universal or Broad crawler}. This category of web crawlers does not confine itself to web pages of a specific topic or domain; instead, they continuously traverse links without limitations, collecting all encountered web pages. Google and Bing are classified as universal search engines. The second category is called \textbf{Preferential crawler (Focused crawler)}. Focused crawlers target specific topics, themes, or domains \cite{kumar2017survey}. They are designed to gather information from a particular domain or subject area, providing specialized search results. Scriburg falls into this category as it only focuses on a subset of links on the internet.

\subsection{Crawler Specifications}
\label{sec:cralwer-specifications}
Crawlers can display a diverse range of features and specifications. Nevertheless, certain essential elements must be incorporated, while others are critical for ensuring a reliable and functional crawler. Further details can be explored in the book referenced as \cite{manning2008}.

\begin{itemize}
\item[] \textbf{Robustness:} Web crawlers can be fragile and easy to break due to the nature of the dynamic contents on the web and the internet connection. Crawlers may encounter broken links, leading to errors and incomplete indexing. Some websites may block or ban crawlers' IP addresses if they perceive them as causing too much traffic or disruption. Web crawlers must identify those edge cases and obstacles and tackle them.

\item[] \textbf{Politeness:} The crawler implementation can be unintentionally dangerous if incorrectly designed. A \textbf{Denial of Service (DoS)} and a \textbf{Distributed Denial of Service (DDoS)} attacks can occur due to an irresponsible crawler implementation. Hence, crawlers must respect website policies and avoid breaking up web services and loading the servers.

\item[] \textbf{Performance and Efficiency:} The crawling system should use various resources, such as processing power, storage capacity, and network bandwidth. Moreover, the crawler should be able to function in distributed microservices across multiple machines. Making it scalable when needed.

\item[] \textbf{Freshness:} Obtaining recent versions of previously accessed pages, ensuring the search index remains updated.
\end{itemize}

\subsection{Crawler Architecture}

Figure \ref{fig:web-crawler-arch} shows a basic crawler architecture. The \textit{Fetch} module communicates with the internet and collects the pages passed by the \textit{URL Frontier} module using HTTP requests. The \textit{URL Frontier} module contains a list of the URLs that need to be fetched by the \textit{Fetch} module. \textit{Parsing} module that takes the page content found by the \textit{Fetch} module and parses the page content to find the following links to be passed to the \textit{URL Frontier} and also to parse any value needed from the page, like text and images. The next step involves filtering the parsed document to eliminate previously visited URLs, duplicate content, and pages prohibited by the website. The \textbf{Domain Name System (DNS)}\footnote{The Domain Name System (DNS) is a distributed naming system for internet resources, linking information to domain names.} resolution module identifies the web server from which to retrieve the page indicated by a given URL \cite{manning2008}. The DNS will be excluded in this thesis and will not be discussed. 

\begin{figure}[h]	
     \centering
     \includegraphics[width=10cm]{figures/crawler_architecture.png}
     \caption{An overview of a web crawler architecture \cite{manning2008}.}
     \label{fig:web-crawler-arch}
\end{figure}

The crawling process begins with adding a seed URL to the \textit{URL Frontier} as a starting point. The crawler retrieves and stores the corresponding page for parsing. The page's textual content, embedded links, and images are extracted during parsing, with the content prepared for use by the search engine's indexer. Each parsed link undergoes filtering to determine if it is eligible for inclusion in the \textit{URL Frontier}.

Following parsing, a filtering process is essential. Firstly, the content's uniqueness is verified using a fingerprint, often a checksum\footnote{A checksum is a numerical value computed from data to verify its integrity by detecting errors or changes in the data.} stored in \textit{Doc FP's} database. Next, newly parsed URLs are filtered based on various criteria, such as excluding URLs outside the target country or restricted URLs. Website administrators can specify additional filtering rules, often outlined in a \textbf{\texttt{robots.txt}}\footnote{A robots.txt file is a text file on a website that instructs web crawlers and search engine robots on which parts of the site should be crawled or excluded from crawling. Source:  \url{https://en.wikipedia.org/wiki/Robots.txt}} file. The \textbf{Robots Exclusion Protocol (\texttt{robots.txt})} file is a widely recognized standard websites use to communicate which parts of the site are accessible to web crawlers and other web robots.

The \texttt{robots.txt} file can be obtained at the start of the crawling process and cached for efficiency, assuming it will not change during crawling. This approach is more efficient than making repeated HTTP requests for the file, reducing the number of requests and server load. Including \texttt{robots.txt} in the crawling process aligns with the politeness guidelines discussed in the crawler specifications section \ref{sec:cralwer-specifications}.

\subsection{Crawler Data Structure}
Scriburg employs two distinct data structures for its crawling implementation: it utilizes \textbf{Breadth First Search (BFS)} and \textbf{Depth First Search (DFS)}.

\textbf{Breadth First Search (BFS):} Considering the link planned for crawling as a \textbf{vertex} (node within a graph), it is worth noting that web pages can be conceptualized as graphs rather than trees. In contrast to trees, graphs can include cycles, which means we might revisit the same vertex. For instance, a basic illustration of this is the home page link, which essentially represents a self-loop\footnote{A self-loop in a graph is an edge that connects a vertex to itself, creating a loop originating and ending at the same point.} in a graph because clicking on it will land us on the same page.

To prevent looping and revisiting already visited vertices (pages in the context of the web), we can maintain two distinct data structures: \textit{"visited"} and \textit{"not-visited"} vertices. The \textit{"visited"} vertices can be stored in a \textit{hashmap} where the link serves as the key, and the Boolean value represents whether the link has already been visited (true for visited, false for not visited). The second data structure is a \textit{queue} containing links that still need to be visited.

\begin{figure}[ht] 
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=0.75\textwidth]{figures/bfs-1.png} 
    \caption{Initial condition} 
    \label{fig:bfs-a} 
    \vspace{4ex}
  \end{subfigure}%% 
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=0.75\textwidth]{figures/bfs-2.png} 
    \caption{Visit the 0 vertex}  
    \label{fig:bfs-b} 
    \vspace{4ex}
  \end{subfigure} 
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=0.75\textwidth]{figures/bfs-3.png} 
    \caption{Visit 1 and 2 vertices} 
    \label{fig:bfs-c} 
  \end{subfigure}%%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=0.75\textwidth]{figures/bfs-4.png} 
    \caption{Completed condition} 
    \label{fig:bfs-d} 
  \end{subfigure} 
  \caption{Illustration of the Breadth First Search (BFS) algorithm visiting five URLs using a queue
 \cite{bfs}.}
  \label{fig:bfs} 
\end{figure}

Figure \ref{fig:bfs} illustrates the BFS algorithm in operation to enhance visual understanding. Let us visualize a seed URL, denoted as vertex 0. The seed URL page contains two additional links, 1 and 2. The page represented by vertex 1 contains three links to pages 0, 2, and 3. Similarly, the page associated with vertex 2 includes three links: 0, 1, and 4. Our primary aim is to visit all nodes (pages), which is the fundamental objective of the crawler. The crawler must avoid infinite looping and avoid revisiting previously visited nodes (pages).

Initially, the \textit{queue} and the \textit{hashmap} are empty of any entries, which is the ideal state of the crawler. As the crawler launches its crawling journey, it begins by pushing the seed URL, node 0, into the \textit{queue}. Then, node 0 will be crawled since it is at the beginning of the \textit{queue} and flagged as visited in the \textit{hashmap}. Subsequently, once the 0 page has been visited, it is dequeued, and the following two discovered links, 1 and 2, are added to the queue. These links, 1 and 2, are similarly dequeued and recorded in the \textit{hashmap} as visited links. This process persists until the queue is empty, ensuring all the links (nodes) have been visited. It is essential to observe that when the crawler encounters a loop or a link pointing to a previously visited link, we can verify if the link has been marked as visited in the \textit{hashmap} before pushing it to the \textit{queue}.

In the case of a random graph, the time complexity of BFS is denoted as $O(|V|+|E|)$ where $|V|$ is the number of vertices and $|E|$ is the number of edges in the graph \cite{cormen01introduction}. This time complexity depends on the graph's topology, where $O(|E|)$ can range from $O(|V|)$ (in the scenario of an acyclic graph) to $O(|V|^2)$ (if all vertices are interconnected). Consequently, the time complexity fluctuates between $O(|V| + |V|) = O(|V|)$ and $O(|V| + |V|^2) = O(|V|^2)$, depending on the specific topology of the graph. The graph has an acyclic structure in our implementation since it prevents revisiting previously visited links. Consequently, the time complexity for the crawler is $O(|V|)$.
\begin{figure}[ht] 
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=0.75\textwidth]{figures/dfs-1.png} 
    \caption{Initial condition} 
    \label{fig7:a} 
    \vspace{4ex}
  \end{subfigure}%% 
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=0.75\textwidth]{figures/dfs-2.png} 
    \caption{Visit the 0 vertex}  
    \label{fig7:b} 
    \vspace{4ex}
  \end{subfigure} 
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=0.75\textwidth]{figures/dfs-3.png} 
    \caption{Visit 1 and 2 vertices} 
    \label{fig7:c} 
  \end{subfigure}%%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=0.75\textwidth]{figures/dfs-4.png} 
    \caption{Completed condition} 
    \label{fig7:d} 
  \end{subfigure} 
  \caption{Illustration of the Depth First Search (DFS) algorithm visiting five URLs using a stack
 \cite{dfs}.}
  \label{fig:dfs} 
\end{figure}

\textbf{Depth First Search (DFS):} It operates similarly to Breadth First Search (BFS). However, instead of visiting the nodes (pages) discovered first, it explores the most recently discovered nodes. Unlike BFS, DFS can be implemented using a \textit{stack}. Figure \ref{fig:dfs} visually represents how DFS operates while crawling a website.

The crawler begins with the seed URL node, labeled 0, where the crawler first explores the seed URL node 0 and identifies the links within it (1, 2, and 3). Each of these links is added to the \textit{stack}, and after node 0 is visited, we flag it as \textit{"visited"} in the \textit{"visited"} \textit{hashmap}. The next node to be visited is node 1. Since it does not lead to any further linked nodes, the crawler proceeds to the next node in the \textit{stack}, node 2. Upon visiting node 2, it becomes noticeable that it contains an additional link, denoted as 4. In contrast to BFS, which would visit node 3 in this scenario, DFS prioritizes node 4 before 3 due to its use of a \textit{stack} rather than a \textit{queue}. Similar to the BFS example, we use a \textit{hashmap} to keep tracking the visited links to avoid loops. Like BFS, DFS has a time complexity of $O(|V|)$.

\section{Indexing}
\label{sec:indexing}
Within a search engine system, the indexer plays a crucial role in examining and structuring the content found in web pages or documents. Its primary function is to generate an index, an organized data structure that facilitates rapid and effective retrieval of pertinent information when users initiate search queries.

The indexer breaks the content into smaller components, words, and phrases, known as \textbf{tokens}. Afterward, it links these tokens to the respective URLs or documents from which they were created. This structured data is then stored within the index, serving as a vital reference for the search engine. It allows the search engine to swiftly locate and present relevant search results, delivering a seamless and efficient user experience.

\subsection{Tokenization}
Tokenization, within the context of indexing, entails fragmenting a textual document or a text string into smaller components known as tokens. These tokens are typically composed of words or subwords and are the fundamental building blocks for indexing and searching within a text. Tokenization represents a foundational and essential stage in natural language processing. A straightforward approach to tokenization involves dividing the text content based on spaces. For instance, the sentence \textit{"university of freiburg"} would yield these tokens: \textit{"university,"} \textit{"of,"} and \textit{"freiburg."}
While dividing text by spaces is a straightforward and convenient solution, tokenization is a more convoluted task than it initially seems. For instance, words like \textit{"Freiburg?"}, \textit{"Freiburg!"} and \textit{"Freiburg"} should be treated as a single word, \textit{"Freiburg"}. Additionally, words such as \textit{"write,"} \textit{"writes,"} and \textit{"writing"} essentially convey the same meaning and should not be distinguished from one another. Lastly, in cases where phrases like \textit{"Freiburg-University"} are connected by a hyphen, splitting solely by spaces would treat it as a single word, even though it comprises two distinct words.

Various approaches to tokenization exist, and in this thesis, each document undergoes a series of steps:

\begin{itemize}
	\item Initial text segmentation by spaces.
	\item Conversion of all words to lowercase.
	\item Removal of all special characters using regular expressions.
\end{itemize}


For example, the sentence \textit{"What! Is this the University of Freiburg?"} will be transformed after undergoing these processes to \textit{"what,"} \textit{"is,"} \textit{"this,"} \textit{"the,"} \textit{"university,"} \textit{"of,"} and \textit{"freiburg"}. 

\subsection*{Stop Words}
Tokens such as \textit{"the"} and \textit{"of"} in the previous example contribute little importance to the overall outcome of the query because the query is equivalent to \textit{"What University Freiburg."} Although the last query is grammatically incorrect, it contains the most critical tokens to understand the user's intention. This is also why Google understands the user query when it is incomplete.  Eliminating these terms via \textbf{stop words} can lead to excluding certain frequently used words from the indexing process. A \textbf{stop words} list is a list that holds words that can be excluded from the indexing process. Selecting appropriate stop words can enhance search retrieval by using a more compact indexer while also allowing user queries to bypass terms contained in the stop words list. The exclusion of stop words from the indexer can result in more relevant search results, as the search engine can direct its attention to the informative words within the documents.
\subsection{Document Unit}
The term \textbf{document} frequently mentions the specific information intended for retrieval from a web page. While, in some instances, this term encompasses the entirety of a page's content, this holds primarily for universal crawlers like Google. However, in the case of the preferential crawler employed in this thesis, the definition of a document unit is adjustable, depending on the nature of the website and the specific data the user aims to collect. For instance, the document unit may be viewed as a single product listing on an e-commerce website featuring product titles, prices, and descriptions. Contrarily, a news website might treat each article as an individual document. The chapter \ref{chap:approach} will provide more comprehensive guidance on creating a template corresponding to a document.

\subsection{Inverted Index}\label{sec:inverted-index}
Let us define $q$ as the query string provided by the user and $D$ as the documents the user attempts to search in. If we were to implement a simple solution, we would need to iterate over each term in $q$ with a length of $|q|$ and then search each term against every document containing $|D|$ terms. This means three nested for-loop with a cubic complexity of $O(|q| * D * |D|)$.

\lstset{language=python}
{\footnotesize
\begin{lstlisting}[frame=single, caption={A naive way of finding a query match in all the documents.},captionpos=b, label={lst:nested-loops}]
for q_token in q_tokens:
    for document in all_documents:
        for document_token in document.tokens:
            .....
\end{lstlisting}}

 Implementing an inverted index is a more efficient solution to address this issue. An \textbf{inverted index} or inverted file is a data structure used in information retrieval systems, particularly in search engines, to store and efficiently retrieve information about the occurrences of terms (words or phrases) within a collection of documents. It is called \textit{"inverted"} because it inverts the relationship between terms and documents.
In an inverted index, each unique token in the collection of documents is treated as a key, and the value associated with each token is a list of references to the documents where that token appears. This list of references allows for rapid access to all the documents containing a specific token. Some inverted indexes also include the position of the token in the document. 

Creating an Inverted index requires the next steps. The first step is to collect the documents to be indexed. In the context of this thesis, the documents referred to the content inside the crawled pages. The second step is to tokenize the text, turning each document into a list of words known as \textbf{tokens}. The last step is to create a dictionary that maps each term with a list of the document IDs that occurred. The tokenized terms are called dictionaries, and the list of IDs is called \textbf{postings}. 

\begin{figure}[ht]	
     \centering
     \includegraphics[width=10cm]{figures/inverted_index.png}
     \caption{
An illustration of an inverted index featuring three documents. All tokens are included in this example, and the sole text normalization applied is converting all tokens to lowercase. Queries that involve multiple tokens are resolved using intersection operations. However, we are using union operation in Scriburg. \cite{castillo2005effective}}
     \label{fig:inverted-index-docs}
\end{figure}

Given that the inverted list can be implemented as a hashmap or dictionary in Python, where the average time complexity of a hashmap operation is $O(1)$, the process of finding all the documents containing the query tokens $|q|$ has an overall time complexity of $O(|q| * 1)$, which simplifies to $O(|q|)$. The next step involves merging the resulting documents for each token in the query $q$.

Since each token inside the query $q$ will result in a list of documents (postings), the final result should be the union of each list. The number of lists to merge is $|q|$.
Given that the time complexity of merging two sorted lists is $O(n + m)$, where $n$ is the length of the first list, and $m$ is the length of the second list. Since there will be a resulting list (postings) for each term in the query $|q|$, the number of lists to merge is $|q|$. Consequently, the time complexity merging all the postings is $O( |q| * |L|)$, where $|L|$ represents the average length of the posting list.

It is worth noting that $|q|$ is typically small; 40\% of people use two search terms for online search queries in the United States as of January 2020, and only 0.46\% uses ten or more \cite{statista2020}. The essential advantage of using the inverted list is that it makes indexing independent of the document length $|D|$, significantly improving performance.

\section{Ranking}
\label{sec:ranking}
As discussed, the indexing process prepares a dictionary that can be looked up to find relevant tokens that match the search query \textit{q}; however, one needs to rank the returned result based on relevance. For example, a user searching for \textit{"What is Freiburg?"} will be expecting a result about Freiburg city and not to return all documents that contain tokens like \textit{"what"} and \textit{"is,"} which are less important than the most informative term in the sentence which is \textit{"Freiburg."} There are many algorithms for document ranking. However, this thesis will adopt \textbf{BM25}. 

Given a query \textit{q}, containing keywords $q_1, ...,q_n$, the \textbf{BM25} score of a document \textit{d} is:

\begin{equation}
score(d,q) = \hat{tf}.\log_2(\frac{N}{df})
\label{eq:bm25}
\end{equation}
\begin{equation}
\hat{tf} = \frac{tf.(k+1)}{k.\alpha+tf}
\label{eq:tf}
\end{equation}
\begin{equation}
\alpha = \frac{1-b+b.DL}{AVDL}
\label{eq:alpha}
\end{equation}

\textit{\textbf{N}}: total number of documents. \textit{\textbf{tf}}: term frequency, the number of times a word occurs in a document. \textit{\textbf{df}}: document frequency, the number of documents containing a particular word. \textit{\textbf{DL}}: document length. \textit{\textbf{AVDL}}: average document length (measured in a number of words).

The parameter \textbf{\textit{b}} prevents the impact of document length normalization. It is a numeric value within the range of 0 to 1. When \textit{b} is set to 0, there is no normalization, implying that longer documents do not face any penalties. In contrast, a value of 1 indicates complete normalization, where longer documents are penalized in proportion to their length. The default value will be set in the implementation is 0.75.

The parameter \textbf{\textit{k}} governs the impact of term frequency saturation in scoring. It is a positive parameter that dictates the speed at which the term frequency component of the score achieves its peak value. When \textit{k} is set to 0, there is no consideration of term frequency, implying that the score remains unaffected by the number of times query terms appear in the document. On the other hand, when \textit{k} is set to a significant value, more weight is given to term frequency, and the score escalates linearly with term frequency. The default value will be set in the implementation is 1.75.

The following example dives into the details of the \textit{BM25} equation and how it impacts ranking. Table \ref{table:bm25-documents} shows a list of documents as an example of an input to be indexed and ranked against different search queries. We start by calculating the variables needed to find the \textit{BM25} scores for each term in a document.    

Given that we have three documents, we set the variable \textit{N} to 3. In the next step, we calculate each document's length \textit{DF}, resulting in \{$1: 26, 2: 21, 3: 49$\}. The average document length \textit{AVDL} can be calculated as $\frac{26 + 21 + 49}{3} = 32$. Substituting these values into the equation, we obtain the inverted list shown in Table \ref{table:bm25-result}.

\begin{table}[ht]
\centering
{\footnotesize
\begin{tabular}{ |P{2.5cm}|P{9.9cm}|  }
\hline 
\textbf{Document ID} & \textbf{Document Content}\T\B 
\\ 
\hline
1 & The University of Freiburg, officially the Albert Ludwig University of Freiburg, is a public research university located in Freiburg im Breisgau, Baden-Württemberg, Germany.\T\B 
\\ 
\hline
2 & Freiburg im Breisgau, usually called simply Freiburg, is an independent city in the state of Baden-Württemberg in Germany.\T\B 
\\ 
\hline
3 & A university from Latin universitas 'a whole' is an institution of higher (or tertiary) education and research which awards academic degrees in several academic disciplines. Universities typically offer both undergraduate and postgraduate programs. In the United States, the designation is reserved for colleges that have a graduate school.\T\B 
\\ 
\hline 
    \end{tabular}
}
  \captionsetup{justification=centering,margin=2cm}
  \caption{Documents content used as an example of MB25 ranking.}
  \label{table:bm25-documents}
\end{table}

When we analyze the data in Table \ref{table:bm25-result}, it becomes evident that tokens shared by all three documents, such as \textit{"the,"} \textit{"of,"} and \textit{"is,"} have corresponding scores of 0. It is normal to give low scores to tokens that contribute little to no to the query. Consider a scenario where a user inputs the word \textit{"the"} in their query. In this case, every document contains this word, making it challenging to maintain relevance. The only way to exhibit bias toward one document over another is when the token's frequency within a document is significantly higher than the document's length.

On the other hand, unique words like \textit{"albert"} and \textit{"ludwig"} receive high scores since they are exclusive to a single document. Words such as \textit{"freiburg"} and \textit{"university"} have varying scores for each document, depending on their relative occurrences in relation to the document's length.

\begin{table}[ht] 
\centering
{\footnotesize
\begin{tabular}{ |P{2.5cm}|P{9.9cm}|  }
 \hline 
\textbf{Token} & \textbf{Postings: (Doc. ID, BM25 Score)}\T\B 
\\ 
\hline
the & (1, 0), (2, 0), (3, 0) \T\B 
\\ 
\hline
university &  (1, 1.071), (3, 0.466) \T\B 
\\ 
\hline
of  &  (1, 0), (2, 0), (3, 0) \T\B 
\\
\hline
freiburg  &  (1, 1.071), (2, 0.975) \T\B 
\\ 
\hline
officially  &  (1, 1.740) \T\B 
\\ 
\hline
albert  & (1, 1.740)\T\B 
\\ 
\hline
ludwig  &  (1, 1.740) \T\B 
\\ 
\hline
is  & (1, 0), (2, 0), (3, 0) \T\B 
\\ 
\hline
a  & (1, 0.642), (3, 0.885) \T\B 
\\ 
\hline
public  &  (1, 1.740) \T\B 
\\ 
\hline
    \end{tabular}
}
  \captionsetup{justification=centering,margin=2cm}
  \caption{The first ten tokens from the resulting inverted index and the corresponding document scores. }
  \label{table:bm25-result}
\end{table}

A user's query for \textit{"university of freiburg"} will yield the following results: \{(1, 2.142), (2, 0.975), (3, 0.466)\}. The initial document with ID 1 receives the highest score (2.142) since it encompasses all three query terms (\textit{"university,"} \textit{"of,"} and \textit{"freiburg"}). This outcome aligns with expectations, given that the first document is indeed about the University of Freiburg.

However, the concern regarding relevancy lies in the two primary keywords, \textit{"university"} and \textit{"freiburg,"} mentioned in documents 2 and 3. The question then becomes, which documents should be prioritized in this scenario? This distinction can be influenced by adjusting the values of the \textit{b} and \textit{k} parameters. In this case, document 2 takes importance over document 3 because the term \textit{"freiburg"} is repeated twice, and the document itself is shorter than document 3.


\section{Fuzzy Search}
Frequently, users make spelling errors in their input queries or may employ American English spellings, such as \textit{"color,"}, which is equivalent to British English spellings \textit{"colour."} It would be less than ideal to return no results solely because the user chose a word variant over another. Other situations may occur where users need clarification about the correct spelling of a new term or someone's name. In Scriburg, we will use the \textbf{fuzzy search} feature to offer users suggestions as they type their queries and to identify the closest matching query entered by the user.

\textbf{Fuzzy search} is a technique used in natural language processing (NLP) and information retrieval to find approximate matches for a given query or search term, even when the exact spelling or wording might not be present in the target text. This is particularly useful when dealing with typos, misspellings, phrasing variations, or other minor deviations from the original text.

In Scriburg, the fuzzy search method employed for spelling correction is labeled as \textbf{isolated-term} \cite{manning2008}. \textbf{Isolated-term} focuses on correcting individual query terms one by one rather than fixing the entire sentence within a contextual context.

Fuzzy search algorithms typically involve techniques like \textbf{Levenshtein distance (Edit Distance ED)}, which calculates the minimum number of single-character edits (insertions, deletions, substitutions) required to transform one string into another. Other techniques include using phonetic algorithms to find similar-sounding words or tokenization and comparing word \textbf{q-grams} to identify overlapping substrings. The greater the shared substring between the two texts, the more likely this is the intended word the user was trying to search for. More details on q-grams will be discussed in section \ref{sec:q-gram-fuzzy}.


Considering two character strings, $s_1$ and $s_2$, the edit distance that separates them represents the minimal count of edit operations needed to transform $s_1$ into $s_2$. The typical edit operations permitted for this purpose include inserting a character into a string, deleting a character from a string, and replacing a character within a string with another character. In the context of these operations, the term \textit{"Levenshtein distance"} is sometimes used interchangeably with \textit{"Edit Distance"}. For example, the edit distance between \textit{"black"} and \textit{"back"} is one because we need to remove one letter, the \textit{"l"} from \textit{"black"} to transform it \textit{"back."} 

In Scriburg, we use \textbf{Prefix Edit Distance PED} instead of Edit Distance. Prefix edit distance is a variation of edit distance that focuses on finding the minimum number of edit operations needed to transform one string into a \textbf{prefix} of the other. The prefix edit distance between $s_1$ and $s_2$ is defined as $PED(s_1, s_2) = min_{s_2^{'}} ED(s_1, s_2^{'})$ where $s_2^{'}$ is a prefix of $s_2$ \cite{freiburg2023ir}.


Fuzzy search requires the existence of a \textbf{dictionary}, which comprises a collection of words for conducting searches to locate the closest match and then return the result. The creation of this dictionary can take various forms, and in the case of Scriburg, we employ two distinct dictionaries.

The initial dictionary is the same one used during the document indexing process when crawling the web. This choice derives from our desire to ensure that the words indexed from the crawled documents can be effectively looked up.

The second dictionary, which is a user predefined, is presented by using \textbf{Wikidata} in Scriburg. This dictionary is particularly valuable in scenarios where synonyms are employed. For instance, if a user inputs \textit{"USA,"} a dropdown menu will display \textit{"United States"} as a suggestion.

In the Scriburg user interface) we offer users the flexibility to select the dictionary of their choice. This option is essential, especially in specific domains where using a particular dictionary is crucial for accurate results. The dictionary construction process is carried out before indexing the crawled documents.

\subsection{Fuzzy Search With Q-grams}\label{sec:q-gram-fuzzy}
Calculating the prefix edit distance for a given word within a dictionary is computationally costly and can be accelerated. One approach involves using \textbf{q-grams}. For a given string \textit{s} and a natural number $n \in N$,  we form the multiset of q-grams, represented as $Q_n(s)$, which encompasses all substrings of length $n$ (Freiburg, 2023).


For example if \textit{s = "freiburg"} and \textit{n = 3} then the resulting q-grams are:

\begin{equation*}
Q_3("freiburg") = \{ "fre", "rei", "eib", "ibu", "bur", "urg"\}
\end{equation*}

Where the number of q-grams of a string s can be calculated as:

\begin{equation*}
|Q_n(s)| = |s| - n + 1 = 8 - 3 + 1 = 6
\end{equation*}

\subsection{Q-gram Index}
We have already explained what an \textbf{inverted index} is in section \ref{sec:inverted-index}, where we explained that tokens derived from documents would be organized within a dictionary or hashmap. This q-gram dictionary is structured with q-grams as keys and values containing lists of tokens from the documents, along with the corresponding q-gram frequencies.


\begin{figure}[h]	
     \centering
         \includegraphics[width=10cm]{figures/q-gram.png}
              \caption{A simple q-gram dictionary where three q-grams are linked to the tokens that contain them \cite{manning2008}.}
     \label{fig:q-gram-example}
\end{figure}

Figure \ref{fig:q-gram-example} presents a visual representation illustrating the fundamental concept of the q-gram inverted list. It is essential to clarify that in the actual implementation, we store references to the tokens associated with the q-gram rather than the tokens themselves as strings. Furthermore, we will also record the frequency of occurrences of the q-gram within a token. The fuzzy search implementation is rooted in the principles of the Information Retrieval course \cite{freiburg2023ir}. It is highly recommended to review the course lectures for a more detailed understanding of its functionality and implementation guidance.