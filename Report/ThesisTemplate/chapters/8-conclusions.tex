\chapter{Conclusions and Future Work}
\label{chap:conclusion}
In this thesis, I delved deeply into the complexities of building a search engine and crafting a crawling and indexing process suitable for distributed cluster operation. The crawling phase presented the most formidable challenges among these due to its various edge cases. Achieving the right balance between being a fast crawler and maintaining proper politeness to avoid overloading servers and causing potential Denial of Service (DoS) issues proved one of the most demanding aspects. Additionally, the prevalence of high-speed internet applications posed a tough challenge in creating a crawler capable of effectively handling diverse use cases.

Crawling extensive websites encompassing millions of documents often necessitates multiple machines, as relying on a single machine can be problematic. Fortunately, many gigantic websites offer APIs to access their database information, presenting a viable alternative to crawling.

While evaluating Douglas and StackOverflow, we encountered issues such as IP address bans. To address this, we mitigated the problem by reducing the HTTP request rate by reducing the number of threads. However, other services like ParseHub circumvent this issue by implementing IP Rotation. This technique, while advantageous, typically requires users to invest in an IP pool for rotation purposes. Incorporating this feature into our crawler is a potential enhancement, though it necessitates additional consideration, including cost implications.

I suggest adding IP Rotation if it is affordable and always remaining polite for future work. Furthermore, I am enthusiastic about enhancing our inspection tool to be more user-friendly and accessible, similar to the one utilized in ParseHub. Additionally, I find it fascinating to introduce new features, such as a health bar, to offer users quick insights into the progress of their crawling processes. While we currently display status codes, errors, and helpful statistics, I aim to simplify this information into a more intuitive visualization.

Another route for improvement involves the development of a tailored protocol that both crawlers and websites can attach to. While using the Robots.txt file is a positive step, there is potential to expand and refine this approach further.


\bibliography{References}
