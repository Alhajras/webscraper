\chapter{Introduction}
\label{chap:introduction}
\section{Motivation}


Since the beginning of the Digital Revolution, known as the Third Industrial Revolution, in the latter half of the 20th century, the importance of data has increased as it became the new currency shaping the dynamics of our interconnected world. From social media platforms and e-commerce transactions to information sharing and entertainment consumption, online activities generate enormous amounts of data. The online data is sometimes referred to as the 'new oil' or the 'new currency', as it impacts almost the same economies and societies as oil. Businesses and organizations understand the power of data as they provide insight into consumer behaviour, refines business strategies, and enhance decision-making processes. Furthermore, the rise of artificial intelligence has further amplified the value of Internet data. Natural language processing (NLP) is becoming a new hot topic as all the giant firms race to create their model; however, data is the fuel to power those models. The more data is collected, the better the model can become. Consequently, collecting, analyzing, and leveraging internet data has become a cornerstone of competitiveness, innovation, and progress in the digital age.

The Internet data can be harvested by using automated software programs called Web crawlers, also known as web spiders or web bots. Their main goal is to discover, retrieve, and index information from websites.

Internet data can be harvested by using automated software programs called Web crawlers, also known as web spiders or web bots. Their main goal is to discover, retrieve, and index information from websites. The applications and use cases of internet crawlers are diverse and valuable, to name a few: 

\begin{itemize}
  \item Search Engines: Crawlers are essential components to build any search engine, such as Google, Bing, and Yahoo. Crawlers are run on supercomputers to crawl all the content on the internet index web pages and gather information about content, keywords, and links. This data is then used to rank and display search results, ensuring users can quickly find relevant information.
  \item Market Research: Businesses use web crawlers to collect data about their competitors, market trends, and consumer opinion. This information helps in making informed business decisions.
  \item Fraud Detection: Cybersecurity companies use crawlers to catch fraudulent activities by monitoring online transactions, identifying unusual patterns, and tracking potential threats.
  \item Content Monitoring: E-commerce platforms utilize crawlers to extract product prices from various websites. This enables them to offer consumers real-time price comparisons and assist in finding the best deals. Moreover, social media platforms use crawlers to monitor their content to prevent unwanted posts and images.
\end{itemize}

The World Wide Web (WWW) contains an enormous amount of data; this data is increasing each day rapidly. The amount of total data created and replicated is expected to grow to more than 180 zettabytes by 2025 according to Statista. The growth is expected to continue as more smartphones are more and more affordable, and more people can reach the internet. Moreover, due to the COVID-19 pandemic, more companies started offering work remotely, more shops created online stores, and more services switched to cloud-based. This change in society during the last few years has made the internet a vital part of our day-to-day life.   

Although the data is available, making a helpful meaning is a challenge. Search engines, for example, try to organize and index that information to make them easily searchable by the end user. Furthermore, collecting data can help spot competitors and have a deeper meaning in the market. Additionally, data scientists are now playing essential roles in most organizations and enterprises to understand consumer needs by collecting and analyzing data from the web.  


Although some websites provide APIs to provide organized information about their services, for example, some airline companies provide API that serves information about their flight schedules, other online shops also provide a documented API to get helpful information about their available products. This is not a guaranteed approach to gathering data, as not all websites offer an excellent documented API. For example, social media websites are reluctant to give information about their users, which is understandable. What if you would like to go through all comments and classify them as spam or not? Depending only on the assumption of having an API for each website is a fragile approach. 

Information retrieval (IR) is a term introduced in 1951 by Calvin
Mooers. It is accessing and retrieving data from a vast pool of unstructured information. One of the most practical applications of IR is to collect information from the internet; therefore, implementing a generic algorithm to gather the needed information and index them is a valid approach. Crawlers or Spiders are bots programmed to follow specific roles defined by the user to automate fetching and extracting data from the internet.  

One form of IR is a web search engine. A web search engine is a system engineered to index the internet. Users can search for articles, documents and pages by entering keywords. The search will provide a list of the most related result that matches the search query. Using the crawlers explained earlier; the engine can index the collected information and optimize the search process using different algorithms and techniques. 

Almost everyone nowadays uses Google, Bing or DuckDuckGo search engines for personal usage for research or enterprise to do market research. Search engines are so important that they make Search Engine Optimization SEO position merge and vital to any business. Harvesting, manipulating, and analysing data are essential, making information almost the new currency. 

\section{Problem Statement}

Although the available search engines are too efficient in crawling and indexing the Internet, businesses like E-commerce are primarily interested in knowing the lowest price for a specific product to understand their marketplace among their competitors. Achieving this by using Google, for example, will not solve the issue as the search directly for the product will rank the products based on some criteria predefined by the vendor Google. Those criteria can include best brands, geo-location close to the user, how well the developers optimize the SEO in the website and more. Note that the lowest price criteria are not included in page ranking. The second issue is the result format; each search engine provides a different list of results based on their implementation. This is only suitable if one is only interested in comparing prices and does not care about the various templates used on each website.\\

Search engines need to be tweaked and configured to match the domain of interest as E-commerce in the previous example and to match a specific use case like the price comparison mentioned.\\

The main problem is that businesses are often interested in only a portion of the internet that interset with their domain and expertise. Furthermore, the criteria for indexing and page ranking depend heavily on their use case and is vital to their business to take control of it and configure it as fit. Hiring domain expertes is invitable to any busness, however the data sceintest often have to go throw some steps to get their crawler up and running, those steps cost money and time,it would be helpful to have an infrastructure that allows the data scientist to have starting script that an be extended easly and needs little to no programming knolwedge. 




Amount of data created, consumed, and stored 2010-2020, with forecasts to 2025
Published by 
Petroc Taylor
, Sep 8, 2022
https://www.statista.com/statistics/871513/worldwide-data-created/#:~:text=The%20total%20amount%20of%20data,to%20more%20than%20180%20zettabytes.
\section{Contribution}

This thesis aims to propose a soltoin on how to create a web search engine software that have the following requiremtns: 
\begin{itemize}
  \item A User Interface UI that allows users crawl and index targeted websites from the internet.
\item The desgin and implmentation should consider generic websites and not to be optimized to only one domain.
    \item The desgin and implmentation should consider making the crawling and indexing process configurable as much as possible.
    \item The implmentation should be scalable and extendable, this makes adding more nodes to cralw more information feasable.
\end{itemize}
\section{Chapter Overview}
